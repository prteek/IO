{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from collections import Counter\n",
    "from geopy import Nominatim\n",
    "\n",
    "# Note if running from binder the text file below will give an \n",
    "try:\n",
    "    with open(\"twitter_credentials.txt\", \"r\") as f:\n",
    "        line_text = [line.strip() for line in f]\n",
    "except:\n",
    "    print(\"Note: The credentials file is not available on Git.\\n\"\n",
    "          \"If running the script on Binder, specify own app KEY and SECRET below \"\n",
    "          \"and the code should run without any issue.\\n\")\n",
    "    print(\"go to: https://developer.twitter.com/en/apps \\nand create an app to generate KEY and SECRET\\n\")\n",
    "    print(\"Binder will not store these keys and everything will be reset when the file is closed.\")\n",
    "    \n",
    "    \n",
    "CONSUMER_KEY         = line_text[0]\n",
    "CONSUMER_SECRET      = line_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a search query\n",
    "\n",
    "refer: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets for formatting the search query and understanding results format.  \n",
    "\n",
    "Max. num of results restricted to 100 per search query so we loop over many times and make the same query.\n",
    "But to avoid the results from repeating, we change the max_id of search results after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London, Greater London, England, SW1A 2DX, UK \n",
      "\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# User inputs\n",
    "COUNT_OF_TWEETS_TO_BE_FETCHED = 1000\n",
    "search_string                 = \"avengers\" # \"\" to search everything\n",
    "type_of_result                = \"all\" # all, mixed, recent or popular\n",
    "location_of_interest          = \"London\"\n",
    "radius_of_interest_in_miles   = 50\n",
    "\n",
    "\n",
    "\n",
    "# initialisation\n",
    "twitter        = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "tweets         = []  \n",
    "\n",
    "word_list             = []\n",
    "hashtag_list          = []\n",
    "retweet_count_list    = []\n",
    "favorite_count_list   = []\n",
    "tweet_url_list        = []\n",
    "\n",
    "# Search area definition\n",
    "geolocator        = Nominatim(user_agent='GoogleV3')\n",
    "location          = geolocator.geocode(location_of_interest)\n",
    "print(location,\"\\n\")\n",
    "geo_code = str(location.latitude) + \",\" + str(location.longitude) + \",\" + str(radius_of_interest_in_miles) + \"mi\"\n",
    "\n",
    "\n",
    "num_results_per_query = min([COUNT_OF_TWEETS_TO_BE_FETCHED, 100])\n",
    "MAX_ATTEMPTS          = max(50, COUNT_OF_TWEETS_TO_BE_FETCHED//num_results_per_query)\n",
    "                           \n",
    "for i in range(0,MAX_ATTEMPTS):\n",
    "    if(COUNT_OF_TWEETS_TO_BE_FETCHED < len(tweets)):\n",
    "        break # we got 500 tweets... !!\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # STEP 1: Query Twitter\n",
    "    # STEP 2: Save the returned tweets\n",
    "    # STEP 3: Get the next max_id\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    # STEP 1: Query Twitter\n",
    "    if(0 == i):\n",
    "        # Query twitter for data. \n",
    "        results = twitter.search(q=search_string, count=str(num_results_per_query), geocode=geo_code, \n",
    "                                 result_type=type_of_result)\n",
    "    else:\n",
    "        # After the first call we should have max_id from result of previous call. Pass it in query.\n",
    "        results = twitter.search(q=search_string,count=str(num_results_per_query), geocode=geo_code, \n",
    "                                 result_type=type_of_result,\n",
    "                                 include_entities='true',max_id=next_max_id)\n",
    "\n",
    "    # STEP 2: Save the returned tweets\n",
    "    for status in results['statuses']:        \n",
    "        user = status[\"user\"][\"screen_name\"].encode(\"utf-8\")\n",
    "        user = user.decode(\"utf-8\") # to convert the encoded byte type into string\n",
    "        text = status[\"text\"].encode(\"utf-8\")\n",
    "        text = text.decode(\"utf-8\") # to convert the encoded byte type into string\n",
    "        for word in text.split():\n",
    "            word_list.append(word)\n",
    "            \n",
    "            if word.startswith(\"#\"):\n",
    "                hashtag_list.append(word)\n",
    "        \n",
    "        tweets.append(text) # Keep track of number of tweets\n",
    "        favorite_count_list.append(status[\"favorite_count\"])\n",
    "        retweet_count_list.append(status[\"retweet_count\"])\n",
    "        tweet_url_list.append(\"https://twitter.com/i/web/status/\"+status[\"id_str\"])\n",
    "        \n",
    "    # STEP 3: Get the next max_id\n",
    "    try:\n",
    "        # Parse the data returned to get max_id to be passed in consequent call.\n",
    "        next_results_url_params = results['search_metadata']['next_results']\n",
    "        next_max_id = next_results_url_params.split('max_id=')[1].split('&')[0]\n",
    "    except:\n",
    "        # No more next pages\n",
    "        break\n",
    "\n",
    "print(\"...Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets fetched: 3728\n",
      "\n",
      " Top Hashtags:\n",
      "#AvengersEndgame 242\n",
      "#Endgame 122\n",
      "#Avengers 64\n",
      "#avengers 36\n",
      "#ThankYouAvengers 28\n",
      "\n",
      "\n",
      "(most) Retweeted: 9113 \n",
      " RT @danielhowell: successfully booking tickets for endgame was more challenging than the avengers actually defeating thanos \n",
      " https://twitter.com/i/web/status/1122142493196181506\n",
      "\n",
      "\n",
      "(most) Favorited: 1008 \n",
      " i saw iron man 2 for my 13th birthday, the avengers for my 15th and won’t ever forget the first time we saw infinit… https://t.co/ZOUwttV5Mr \n",
      " https://twitter.com/i/web/status/1122097192485949440\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets fetched:\", len(tweets))\n",
    "\n",
    "print(\"\\n Top Hashtags:\")\n",
    "c = Counter(hashtag_list)\n",
    "for tags, count in c.most_common(5):\n",
    "    print(tags,count)\n",
    "    \n",
    "# print(\"\\n Most common words:\")\n",
    "# c = Counter(word_list)\n",
    "# for tags, count in c.most_common(6):\n",
    "#     print(tags,count)\n",
    "\n",
    "print(\"\\n\")\n",
    "max_retweet_index = sorted(range(len(retweet_count_list)), key=lambda x: -retweet_count_list[x])[0]\n",
    "                           \n",
    "most_retweeted    = tweets[max_retweet_index]\n",
    "max_retweet_count = retweet_count_list[max_retweet_index]\n",
    "max_retweet_url   = tweet_url_list[max_retweet_index]   \n",
    "print(\"(most) Retweeted:\", max_retweet_count, \"\\n\", most_retweeted, \"\\n\", max_retweet_url)\n",
    "\n",
    "print(\"\\n\")\n",
    "max_favorite_index = sorted(range(len(favorite_count_list)), key=lambda x: -favorite_count_list[x])[0]\n",
    "most_favorite      = tweets[max_favorite_index]\n",
    "max_favorite_count = favorite_count_list[max_favorite_index]\n",
    "max_favorite_url   = tweet_url_list[max_favorite_index] \n",
    "                            \n",
    "print(\"(most) Favorited:\", max_favorite_count, \"\\n\", most_favorite, \"\\n\", max_favorite_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
