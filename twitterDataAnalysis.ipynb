{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from collections import Counter\n",
    "from geopy import Nominatim\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Note if running from binder the text file below will give an \n",
    "try:\n",
    "    with open(\"twitter_credentials.txt\", \"r\") as f:\n",
    "        line_text = [line.strip() for line in f]\n",
    "except:\n",
    "    print(\"Note: The credentials file is not available on Git.\\n\"\n",
    "          \"If running the script on Binder, specify own app KEY and SECRET below \"\n",
    "          \"and the code should run without any issue.\\n\")\n",
    "    print(\"go to: https://developer.twitter.com/en/apps \\nand create an app to generate KEY and SECRET\\n\")\n",
    "    print(\"Binder will not store these keys and everything will be reset when the file is closed.\")\n",
    "    \n",
    "    \n",
    "CONSUMER_KEY         = line_text[0]\n",
    "CONSUMER_SECRET      = line_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a search query\n",
    "\n",
    "refer: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets for formatting the search query and understanding results format.  \n",
    "\n",
    "Max. num of results restricted to 100 per search query so we loop over many times and make the same query.\n",
    "But to avoid the results from repeating, we change the max_id of search results after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tweets(search_string                 = \"\", # \"\" to search everything\n",
    "                location_of_interest          = \"London\",\n",
    "                radius_of_interest_in_km      = 20,\n",
    "                num_tweets_to_fetch           = 1000,\n",
    "                type_of_result                = \"all\", # all, mixed, recent or popular\n",
    "                c_key=CONSUMER_KEY, c_secret=CONSUMER_SECRET):\n",
    "\n",
    "    # initialisation\n",
    "    twitter        = Twython(c_key, c_secret)\n",
    "\n",
    "    tweets                = []\n",
    "    word_list             = []\n",
    "    hashtag_list          = []\n",
    "    retweet_count_list    = []\n",
    "    favorite_count_list   = []\n",
    "    tweet_url_list        = []\n",
    "\n",
    "    # Search area definition\n",
    "    geolocator        = Nominatim(user_agent='GoogleV3')\n",
    "    location          = geolocator.geocode(location_of_interest)\n",
    "    geo_code = str(location.latitude) + \",\" + str(location.longitude) + \",\" + str(radius_of_interest_in_km) + \"km\"\n",
    "\n",
    "\n",
    "    num_results_per_query = min([num_tweets_to_fetch, 100])\n",
    "\n",
    "    # In case there aren't enough results for the search term\n",
    "    max_attempts          = max(50, num_tweets_to_fetch//num_results_per_query*2) \n",
    "\n",
    "    print(\"fetching...\")   \n",
    "    for i in range(0,max_attempts):\n",
    "        if(num_tweets_to_fetch < len(tweets)):\n",
    "            break # we got all the tweets we asked for ... !!\n",
    "\n",
    "        #----------------------------------------------------------------#\n",
    "        # STEP 1: Query Twitter\n",
    "        # STEP 2: Save the returned tweets\n",
    "        # STEP 3: Get the next max_id\n",
    "        #----------------------------------------------------------------#\n",
    "\n",
    "        # STEP 1: Query Twitter\n",
    "        if(0 == i):\n",
    "            # Query twitter for data. \n",
    "            results = twitter.search(q=search_string, count=str(num_results_per_query), geocode=geo_code, \n",
    "                                     result_type=type_of_result)\n",
    "        else:\n",
    "            # After the first call we should have max_id from result of previous call. Pass it in query.\n",
    "            results = twitter.search(q=search_string,count=str(num_results_per_query), geocode=geo_code, \n",
    "                                     result_type=type_of_result,\n",
    "                                     include_entities='true',max_id=next_max_id)\n",
    "\n",
    "        # STEP 2: Save the returned tweets\n",
    "        for status in results['statuses']:        \n",
    "            user = status[\"user\"][\"screen_name\"].encode(\"utf-8\")\n",
    "            user = user.decode(\"utf-8\") # to convert the encoded byte type into string\n",
    "            text = status[\"text\"].encode(\"utf-8\")\n",
    "            text = text.decode(\"utf-8\") # to convert the encoded byte type into string\n",
    "            for word in text.split():\n",
    "                word_list.append(word)\n",
    "\n",
    "                if word.startswith(\"#\"):\n",
    "                    hashtag_list.append(word)\n",
    "\n",
    "            tweets.append(text) # Keep track of number of tweets\n",
    "            favorite_count_list.append(status[\"favorite_count\"])\n",
    "            retweet_count_list.append(status[\"retweet_count\"])\n",
    "            tweet_url_list.append(\"https://twitter.com/i/web/status/\"+status[\"id_str\"])\n",
    "\n",
    "        # STEP 3: Get the next max_id\n",
    "        try:\n",
    "            # Parse the data returned to get max_id to be passed in consequent call.\n",
    "            next_results_url_params = results['search_metadata']['next_results']\n",
    "            next_max_id = next_results_url_params.split('max_id=')[1].split('&')[0]\n",
    "        except:\n",
    "            # No more next pages\n",
    "            break\n",
    "    \n",
    "    print(\"...Done\")\n",
    "    return tweets, hashtag_list, tweet_url_list, retweet_count_list, word_list, favorite_count_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching...\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# User inputs\n",
    "search_string                 = \"EU\" # \"\" to search everything\n",
    "location_of_interest          = \"London\"\n",
    "radius_of_interest_in_km      = 20\n",
    "num_tweets_to_fetch           = 1000\n",
    "type_of_result                = \"all\" # all, mixed, recent or popular\n",
    "\n",
    "# Search area definition\n",
    "geolocator        = Nominatim(user_agent='GoogleV3')\n",
    "location                      = geolocator.geocode(location_of_interest)\n",
    "\n",
    "tweets, hashtag_list, tweet_url_list, retweet_count_list, word_list, favorite_count_list = find_tweets(search_string, \n",
    "            location_of_interest, \n",
    "            radius_of_interest_in_km,\n",
    "            num_tweets_to_fetch,\n",
    "            type_of_result,\n",
    "            c_key=CONSUMER_KEY, c_secret=CONSUMER_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London, Greater London, England, SW1A 2DX, UK \n",
      "\n",
      "Number of tweets fetched: 1050\n",
      "\n",
      " Top Hashtags:\n",
      "#Brexit. 37\n",
      "#Brexit 25\n",
      "#Remain 17\n",
      "#PeoplesVote 17\n",
      "#EP2019 13\n",
      "\n",
      "\n",
      "(most) Retweeted: 8275 \n",
      " RT @mikegalsworthy: RT if you think no new MEPs should be announced...\n",
      "\n",
      "...until all those EU &amp; British citizens erroneously denied votes h… \n",
      "\n",
      " tweet link: https://twitter.com/i/web/status/1132949503697399808\n",
      "\n",
      "\n",
      "(most) Favorited: 64 \n",
      " How many EU election losses does it take for a Remoaner to change a lightbulb?\n",
      "I'll have to get back to you on that… https://t.co/qICG2EZ1XP \n",
      "\n",
      " tweet link: https://twitter.com/i/web/status/1132951723595767808\n"
     ]
    }
   ],
   "source": [
    "print(location,\"\\n\")\n",
    "print(\"Number of tweets fetched:\", len(tweets))\n",
    "\n",
    "print(\"\\n Top Hashtags:\")\n",
    "c = Counter(hashtag_list)\n",
    "for tags, count in c.most_common(5):\n",
    "    print(tags,count)\n",
    "    \n",
    "# print(\"\\n Most common words:\")\n",
    "# c = Counter(word_list)\n",
    "# for tags, count in c.most_common(6):\n",
    "#     print(tags,count)\n",
    "\n",
    "print(\"\\n\")\n",
    "max_retweet_index = sorted(range(len(retweet_count_list)), key=lambda x: -retweet_count_list[x])[0]\n",
    "                           \n",
    "most_retweeted    = tweets[max_retweet_index]\n",
    "max_retweet_count = retweet_count_list[max_retweet_index]\n",
    "max_retweet_url   = tweet_url_list[max_retweet_index]   \n",
    "print(\"(most) Retweeted:\", max_retweet_count, \"\\n\", most_retweeted, \"\\n\\n\", \"tweet link:\", max_retweet_url)\n",
    "\n",
    "print(\"\\n\")\n",
    "max_favorite_index = sorted(range(len(favorite_count_list)), key=lambda x: -favorite_count_list[x])[0]\n",
    "most_favorite      = tweets[max_favorite_index]\n",
    "max_favorite_count = favorite_count_list[max_favorite_index]\n",
    "max_favorite_url   = tweet_url_list[max_favorite_index] \n",
    "                            \n",
    "print(\"(most) Favorited:\", max_favorite_count, \"\\n\", most_favorite, \"\\n\\n\", \"tweet link:\", max_favorite_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
