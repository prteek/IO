{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from collections import Counter\n",
    "from geopy import Nominatim\n",
    "\n",
    "with open(\"twitter_data_from_scratch.txt\", \"r\") as f:\n",
    "    line_text = [line.strip() for line in f]\n",
    "    \n",
    "\n",
    "CONSUMER_KEY         = line_text[0]\n",
    "CONSUMER_SECRET      = line_text[1]\n",
    "ACCESS_TOKEN         = line_text[2]\n",
    "ACCESS_TOKEN_SECRET  = line_text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a search query\n",
    "\n",
    "refer: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets for formatting the search query and understanding results format.  \n",
    "\n",
    "Max. num of results restricted to 100 per search query so we loop over many times and make the same query.\n",
    "But to avoid the results from repeating, we change the max_id of search results after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London, Greater London, England, SW1A 2DX, UK \n",
      "\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# User inputs\n",
    "COUNT_OF_TWEETS_TO_BE_FETCHED = 1000\n",
    "search_string                 = \"avengers\" # \"\" to search everything\n",
    "type_of_result                = \"all\" # all, mixed, recent or popular\n",
    "location_of_interest          = \"London\"\n",
    "radius_of_interest_in_miles   = 50\n",
    "\n",
    "\n",
    "\n",
    "# initialisation\n",
    "twitter        = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "tweets         = []  \n",
    "\n",
    "word_list             = []\n",
    "hashtag_list          = []\n",
    "retweet_count_list    = []\n",
    "favorite_count_list   = []\n",
    "tweet_url_list        = []\n",
    "\n",
    "# Search area definition\n",
    "geolocator        = Nominatim(user_agent='GoogleV3')\n",
    "location          = geolocator.geocode(location_of_interest)\n",
    "print(location,\"\\n\")\n",
    "geo_code = str(location.latitude) + \",\" + str(location.longitude) + \",\" + str(radius_of_interest_in_miles) + \"mi\"\n",
    "\n",
    "\n",
    "num_results_per_query = min([COUNT_OF_TWEETS_TO_BE_FETCHED, 100])\n",
    "MAX_ATTEMPTS          = max(50, COUNT_OF_TWEETS_TO_BE_FETCHED//num_results_per_query)\n",
    "                           \n",
    "for i in range(0,MAX_ATTEMPTS):\n",
    "    if(COUNT_OF_TWEETS_TO_BE_FETCHED < len(tweets)):\n",
    "        break # we got 500 tweets... !!\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # STEP 1: Query Twitter\n",
    "    # STEP 2: Save the returned tweets\n",
    "    # STEP 3: Get the next max_id\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    # STEP 1: Query Twitter\n",
    "    if(0 == i):\n",
    "        # Query twitter for data. \n",
    "        results = twitter.search(q=search_string, count=str(num_results_per_query), geocode=geo_code, \n",
    "                                 result_type=type_of_result)\n",
    "    else:\n",
    "        # After the first call we should have max_id from result of previous call. Pass it in query.\n",
    "        results = twitter.search(q=search_string,count=str(num_results_per_query), geocode=geo_code, \n",
    "                                 result_type=type_of_result,\n",
    "                                 include_entities='true',max_id=next_max_id)\n",
    "\n",
    "    # STEP 2: Save the returned tweets\n",
    "    for status in results['statuses']:        \n",
    "        user = status[\"user\"][\"screen_name\"].encode(\"utf-8\")\n",
    "        user = user.decode(\"utf-8\") # to convert the encoded byte type into string\n",
    "        text = status[\"text\"].encode(\"utf-8\")\n",
    "        text = text.decode(\"utf-8\") # to convert the encoded byte type into string\n",
    "        for word in text.split():\n",
    "            word_list.append(word)\n",
    "            \n",
    "            if word.startswith(\"#\"):\n",
    "                hashtag_list.append(word)\n",
    "        \n",
    "        tweets.append(text) # Keep track of number of tweets\n",
    "        favorite_count_list.append(status[\"favorite_count\"])\n",
    "        retweet_count_list.append(status[\"retweet_count\"])\n",
    "        tweet_url_list.append(\"https://twitter.com/i/web/status/\"+status[\"id_str\"])\n",
    "        \n",
    "    # STEP 3: Get the next max_id\n",
    "    try:\n",
    "        # Parse the data returned to get max_id to be passed in consequent call.\n",
    "        next_results_url_params = results['search_metadata']['next_results']\n",
    "        next_max_id = next_results_url_params.split('max_id=')[1].split('&')[0]\n",
    "    except:\n",
    "        # No more next pages\n",
    "        break\n",
    "\n",
    "print(\"...Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets fetched: 1011\n",
      "\n",
      " Top Hashtags:\n",
      "#AvengersEndgame 60\n",
      "#Endgame 51\n",
      "#ThankYouAvengers 16\n",
      "#avengers 11\n",
      "#Avengers 9\n",
      "\n",
      "\n",
      "(most) Retweeted: 1972 \n",
      " RT @The_Shiznit: I sat through 12 minutes of the Avengers: Endgame credits and all I was rewarded with was a lousy sense of appreciation fo… \n",
      " https://twitter.com/i/web/status/1122208241675636736\n",
      "\n",
      "\n",
      "(most) Favorited: 316 \n",
      " The regressive Left’s depiction of old white men as evil, diseased oppressors and themselves as righteous avengers… https://t.co/VQOFzTIGuZ \n",
      " https://twitter.com/i/web/status/1122186976541724674\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets fetched:\", len(tweets))\n",
    "\n",
    "print(\"\\n Top Hashtags:\")\n",
    "c = Counter(hashtag_list)\n",
    "for tags, count in c.most_common(5):\n",
    "    print(tags,count)\n",
    "    \n",
    "# print(\"\\n Most common words:\")\n",
    "# c = Counter(word_list)\n",
    "# for tags, count in c.most_common(6):\n",
    "#     print(tags,count)\n",
    "\n",
    "print(\"\\n\")\n",
    "max_retweet_index = sorted(range(len(retweet_count_list)), key=lambda x: -retweet_count_list[x])[0]\n",
    "                           \n",
    "most_retweeted    = tweets[max_retweet_index]\n",
    "max_retweet_count = retweet_count_list[max_retweet_index]\n",
    "max_retweet_url   = tweet_url_list[max_retweet_index]   \n",
    "print(\"(most) Retweeted:\", max_retweet_count, \"\\n\", most_retweeted, \"\\n\", max_retweet_url)\n",
    "\n",
    "print(\"\\n\")\n",
    "max_favorite_index = sorted(range(len(favorite_count_list)), key=lambda x: -favorite_count_list[x])[0]\n",
    "most_favorite      = tweets[max_favorite_index]\n",
    "max_favorite_count = favorite_count_list[max_favorite_index]\n",
    "max_favorite_url   = tweet_url_list[max_favorite_index] \n",
    "                            \n",
    "print(\"(most) Favorited:\", max_favorite_count, \"\\n\", most_favorite, \"\\n\", max_favorite_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
