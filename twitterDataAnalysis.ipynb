{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from collections import Counter\n",
    "from geopy import Nominatim\n",
    "\n",
    "# Note if running from binder the text file below will give an \n",
    "try:\n",
    "    with open(\"twitter_credentials.txt\", \"r\") as f:\n",
    "        line_text = [line.strip() for line in f]\n",
    "except:\n",
    "    print(\"Note: The credentials file is not available on Git.\\n\"\n",
    "          \"If running the script on Binder, specify own app KEY and SECRET below \"\n",
    "          \"and the code should run without any issue.\\n\")\n",
    "    print(\"go to: https://developer.twitter.com/en/apps \\nand create an app to generate KEY and SECRET\\n\")\n",
    "    print(\"Binder will not store these keys and everything will be reset when the file is closed.\")\n",
    "    \n",
    "    \n",
    "CONSUMER_KEY         = line_text[0]\n",
    "CONSUMER_SECRET      = line_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a search query\n",
    "\n",
    "refer: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets for formatting the search query and understanding results format.  \n",
    "\n",
    "Max. num of results restricted to 100 per search query so we loop over many times and make the same query.\n",
    "But to avoid the results from repeating, we change the max_id of search results after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching...\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# User inputs\n",
    "num_tweets_to_fetch           = 1000\n",
    "search_string                 = \"\" # \"\" to search everything\n",
    "type_of_result                = \"all\" # all, mixed, recent or popular\n",
    "location_of_interest          = \"HORNCHURCH High Street\"\n",
    "radius_of_interest_in_km      = 20\n",
    "\n",
    "\n",
    "\n",
    "# initialisation\n",
    "twitter        = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "tweets                = []  \n",
    "word_list             = []\n",
    "hashtag_list          = []\n",
    "retweet_count_list    = []\n",
    "favorite_count_list   = []\n",
    "tweet_url_list        = []\n",
    "\n",
    "# Search area definition\n",
    "geolocator        = Nominatim(user_agent='GoogleV3')\n",
    "location          = geolocator.geocode(location_of_interest)\n",
    "geo_code = str(location.latitude) + \",\" + str(location.longitude) + \",\" + str(radius_of_interest_in_km) + \"km\"\n",
    "\n",
    "\n",
    "num_results_per_query = min([num_tweets_to_fetch, 100])\n",
    "\n",
    "# In case there aren't enough results for the search term\n",
    "max_attempts          = max(50, num_tweets_to_fetch//num_results_per_query*2) \n",
    "\n",
    "print(\"fetching...\")   \n",
    "for i in range(0,max_attempts):\n",
    "    if(num_tweets_to_fetch < len(tweets)):\n",
    "        break # we got all the tweets we asked for ... !!\n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    # STEP 1: Query Twitter\n",
    "    # STEP 2: Save the returned tweets\n",
    "    # STEP 3: Get the next max_id\n",
    "    #----------------------------------------------------------------#\n",
    "\n",
    "    # STEP 1: Query Twitter\n",
    "    if(0 == i):\n",
    "        # Query twitter for data. \n",
    "        results = twitter.search(q=search_string, count=str(num_results_per_query), geocode=geo_code, \n",
    "                                 result_type=type_of_result)\n",
    "    else:\n",
    "        # After the first call we should have max_id from result of previous call. Pass it in query.\n",
    "        results = twitter.search(q=search_string,count=str(num_results_per_query), geocode=geo_code, \n",
    "                                 result_type=type_of_result,\n",
    "                                 include_entities='true',max_id=next_max_id)\n",
    "\n",
    "    # STEP 2: Save the returned tweets\n",
    "    for status in results['statuses']:        \n",
    "        user = status[\"user\"][\"screen_name\"].encode(\"utf-8\")\n",
    "        user = user.decode(\"utf-8\") # to convert the encoded byte type into string\n",
    "        text = status[\"text\"].encode(\"utf-8\")\n",
    "        text = text.decode(\"utf-8\") # to convert the encoded byte type into string\n",
    "        for word in text.split():\n",
    "            word_list.append(word)\n",
    "            \n",
    "            if word.startswith(\"#\"):\n",
    "                hashtag_list.append(word)\n",
    "        \n",
    "        tweets.append(text) # Keep track of number of tweets\n",
    "        favorite_count_list.append(status[\"favorite_count\"])\n",
    "        retweet_count_list.append(status[\"retweet_count\"])\n",
    "        tweet_url_list.append(\"https://twitter.com/i/web/status/\"+status[\"id_str\"])\n",
    "        \n",
    "    # STEP 3: Get the next max_id\n",
    "    try:\n",
    "        # Parse the data returned to get max_id to be passed in consequent call.\n",
    "        next_results_url_params = results['search_metadata']['next_results']\n",
    "        next_max_id = next_results_url_params.split('max_id=')[1].split('&')[0]\n",
    "    except:\n",
    "        # No more next pages\n",
    "        break\n",
    "\n",
    "print(\"...Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Street, Hornchurch, London Borough of Havering, London, Greater London, England, RM12 6QU, UK \n",
      "\n",
      "Number of tweets fetched: 1032\n",
      "\n",
      " Top Hashtags:\n",
      "#LondonMarathon 9\n",
      "#COYI 5\n",
      "#returnofthefabric4 5\n",
      "#AFCW 4\n",
      "#AFCWimbledon 4\n",
      "\n",
      "\n",
      "(most) Retweeted: 3944 \n",
      " RT @ffsDaley: It's not worth the risk. https://t.co/83diGUiKyX \n",
      "\n",
      " tweet link: https://twitter.com/i/web/status/1122341471401783296\n",
      "\n",
      "\n",
      "(most) Favorited: 34 \n",
      " It doesn’t feel right to stay in a hotel while an mcq is happening. \n",
      "\n",
      "Apologies in advance if I’m more quiet than m… https://t.co/uoax9wl8XP \n",
      "\n",
      " tweet link: https://twitter.com/i/web/status/1122358864630779904\n"
     ]
    }
   ],
   "source": [
    "print(location,\"\\n\")\n",
    "print(\"Number of tweets fetched:\", len(tweets))\n",
    "\n",
    "print(\"\\n Top Hashtags:\")\n",
    "c = Counter(hashtag_list)\n",
    "for tags, count in c.most_common(5):\n",
    "    print(tags,count)\n",
    "    \n",
    "# print(\"\\n Most common words:\")\n",
    "# c = Counter(word_list)\n",
    "# for tags, count in c.most_common(6):\n",
    "#     print(tags,count)\n",
    "\n",
    "print(\"\\n\")\n",
    "max_retweet_index = sorted(range(len(retweet_count_list)), key=lambda x: -retweet_count_list[x])[0]\n",
    "                           \n",
    "most_retweeted    = tweets[max_retweet_index]\n",
    "max_retweet_count = retweet_count_list[max_retweet_index]\n",
    "max_retweet_url   = tweet_url_list[max_retweet_index]   \n",
    "print(\"(most) Retweeted:\", max_retweet_count, \"\\n\", most_retweeted, \"\\n\\n\", \"tweet link:\", max_retweet_url)\n",
    "\n",
    "print(\"\\n\")\n",
    "max_favorite_index = sorted(range(len(favorite_count_list)), key=lambda x: -favorite_count_list[x])[0]\n",
    "most_favorite      = tweets[max_favorite_index]\n",
    "max_favorite_count = favorite_count_list[max_favorite_index]\n",
    "max_favorite_url   = tweet_url_list[max_favorite_index] \n",
    "                            \n",
    "print(\"(most) Favorited:\", max_favorite_count, \"\\n\", most_favorite, \"\\n\\n\", \"tweet link:\", max_favorite_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
